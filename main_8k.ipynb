{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb9a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec4a96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow._api.v2.version' from 'C:\\\\Users\\\\Oshin\\\\.virtualenvs\\\\major_project-ADnFf6-K\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v2\\\\version\\\\__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f8fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset_path = './8k/Images/'\n",
    "caption_dataset_path = './8k/captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6fd4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the caption file & read it\n",
    "def load_caption_file(path):\n",
    "    \n",
    "    # dictionary to store captions\n",
    "    captions_dict = {}\n",
    "    \n",
    "    # iterate through the file\n",
    "    for caption in open(path,encoding = 'utf-8'):\n",
    "    \n",
    "        # caption has format-> 1000268201_693b08cb0e.jpg#0  A child in a pink dress is climbing up a set of stairs in an entry way .\n",
    "        tokens = caption.split()\n",
    "        caption_id, caption_text = tokens[0].split('.')[0], tokens[1:]\n",
    "        caption_text = ' '.join(caption_text)\n",
    "        \n",
    "        # save it in the captions dictionary\n",
    "        if caption_id not in captions_dict:\n",
    "            captions_dict[caption_id] = caption_text\n",
    "        \n",
    "    return captions_dict\n",
    "\n",
    "# call the function\n",
    "captions_dict = load_caption_file(caption_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5439ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions_dict = dict(\n",
    "#     list(captions_dict.items())[:10000]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb9a8b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of captions: 8091\n"
     ]
    }
   ],
   "source": [
    "#clean the captions\n",
    "import string\n",
    "\n",
    "# dictionary to store the cleaned captions\n",
    "cleaned_captions_dict = {}\n",
    "\n",
    "# prepare translation table for removing punctuation. third argument is the list of punctuations we want to remove\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# loop through the dictionary\n",
    "for caption_id, caption_text in captions_dict.items():\n",
    "    # tokenize the caption_text\n",
    "    caption_text = caption_text.split()\n",
    "    # convert it into lower case\n",
    "    caption_text = [token.lower() for token in caption_text]\n",
    "    # remove punctuation from each token\n",
    "    caption_text = [token.translate(table) for token in caption_text]\n",
    "    # remove all the single letter tokens like 'a', 's'\n",
    "    caption_text = [token for token in caption_text if len(token)>1]\n",
    "    # store the cleaned captions\n",
    "    cleaned_captions_dict[caption_id] = 'startseq ' + ' '.join(caption_text) + ' endseq'\n",
    "\n",
    "# delete unwanted\n",
    "del(captions_dict)\n",
    "print(f\"length of captions: {len(cleaned_captions_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bce1d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# create word count dictionary on the captions list\n",
    "tokenizer.fit_on_texts(list(cleaned_captions_dict.values()))\n",
    "\n",
    "# how many words are there in the vocabulary? store the total length in vocab_len and add 1 because word_index starts with 1 not 0 \n",
    "vocab_len = len(tokenizer.word_index) + 1\n",
    "\n",
    "# store the length of the maximum sentence\n",
    "# max_len = max(len(train_validate_image_caption[image].split()) for image in train_validate_image_caption)\n",
    "max_len = max(len(cleaned_captions_dict[image].split()) for image in cleaned_captions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fb83086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1000268201_693b08cb0e', 'startseq child in pink dress is climbing up set of stairs in an entry way endseq')\n"
     ]
    }
   ],
   "source": [
    "train_caption = list(cleaned_captions_dict.items())[0:6000]\n",
    "validate_caption = list(cleaned_captions_dict.items())[6000:8091]\n",
    "\n",
    "# train_caption = list(cleaned_captions_dict.items())[0:9000]\n",
    "# # validate_caption = list(cleaned_captions_dict.items())[14500:15000]\n",
    "\n",
    "print(train_caption[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036c0067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "vgg_model = VGG16()\n",
    "\n",
    "# re-structure the model\n",
    "vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_single_features(directory, img_name, ext=\"jpg\"):\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    \n",
    "    # load an image from file\n",
    "    filename = directory + '/' + img_name + \".\" + ext\n",
    "    #filename = directory + '/' + img_name + '.jpeg'\n",
    "    \n",
    "\n",
    "    # load the image and convert it into target size of 224*224\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    # get features\n",
    "    feature = vgg_model.predict(image, verbose=0)\n",
    "\n",
    "    # get image id\n",
    "    image_id = img_name.split('.')[0]\n",
    "\n",
    "    # store feature\n",
    "    features[image_id] = feature\n",
    "\n",
    "    # print('>%s' % name)\n",
    "        \n",
    "    return feature\n",
    "\n",
    "# extract_single_features(image_dataset_path, train_caption[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08a3b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6000/6000 [14:53<00:00,  6.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2091/2091 [05:06<00:00,  6.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def encode_data(data):\n",
    "    encoded_data = {}\n",
    "    for img_id, img_caption in tqdm(data):\n",
    "        encoded_data[img_id] = extract_single_features(image_dataset_path, img_id)\n",
    "    return encoded_data\n",
    "\n",
    "train_features = encode_data(train_caption)\n",
    "validate_features = encode_data(validate_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f01efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open(\"./img_features/15k_train_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_features, f)\n",
    "\n",
    "with open(\"./img_features/15k_validate_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(validate_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54bd462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./img_features/15k_train_features.pkl\", \"rb\") as f:\n",
    "    train_features  = pickle.load(f)\n",
    "\n",
    "with open(\"./img_features/15k_validate_features.pkl\", \"rb\") as f:\n",
    "    validate_features  = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c4dd332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Converted 4198 words (269 misses)\n"
     ]
    }
   ],
   "source": [
    "# Load all the embeddings from the text file\n",
    "def get_embedding_matrix():    \n",
    "    embedding_idx = {}\n",
    "    embedding_matrix = np.zeros((vocab_len, 100))\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    with open(\"./glove_embeddings/glove.6B.100d.txt\",encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embedding_idx[word] = coefs\n",
    "    print(\"Found %s word vectors.\" % len(embedding_idx))\n",
    "\n",
    "\n",
    "    # Make a embedding matrix with embeddings of all the vocabulary\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embedding_idx.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    \n",
    "    return embedding_matrix\n",
    "    \n",
    "embedding_matrix = get_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def dataGen(img_caption, feature):\n",
    "    dataset_size = len(img_caption)\n",
    "    print(\"dataset_size\", dataset_size)\n",
    "    count = 0\n",
    "    # x1 will store the image feature, x2 will store one sequence and y will store the next sequence\n",
    "    x1, x2, y = [], [], []\n",
    "    \n",
    "    for i in tqdm(range(dataset_size)):\n",
    "        image, caption = img_caption[i]\n",
    "        caption = caption.split()\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "        length = len(seq)\n",
    "\n",
    "        # Extract image feature\n",
    "        img_feature = feature[image]\n",
    "\n",
    "        for i in range(1, length):\n",
    "            x2_seq, y_seq = seq[:i] , seq[i]\n",
    "#             print(\"normal x2_seq\", x2_seq)\n",
    "#             print(\"normal y_seq\", y_seq)\n",
    "\n",
    "            # pad the sequences\n",
    "            x2_seq = pad_sequences([x2_seq], maxlen = max_len, padding=\"post\", truncating=\"post\")[0]\n",
    "#             print(\"padded\", x2_seq)\n",
    "            \n",
    "            # encode the output sequence                \n",
    "            y_seq = to_categorical([y_seq], num_classes = vocab_len)[0]\n",
    "#             print(\"categorical y_seq\", y_seq)            \n",
    "\n",
    "            # x1.append( train_validate_features[image][0])\n",
    "            x1.append(img_feature[0])\n",
    "\n",
    "            x2.append(x2_seq)\n",
    "\n",
    "            y.append(y_seq)\n",
    "            count = count + 1\n",
    "    \n",
    "    print(\"Size of dataset: \", count)\n",
    "    return [np.array(x1), np.array(x2)], np.array(y)\n",
    "#     yield [np.array(x1), np.array(x2)], np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268adfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Generator for generating the training data\n",
    "trainX, trainY = dataGen(train_caption, train_features)\n",
    "valX, valY = dataGen(validate_caption, validate_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdace74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor model\n",
    "input_1 = Input(shape=(4096,))\n",
    "droplayer = Dropout(0.5)(input_1)\n",
    "denselayer = Dense(256, activation='relu')(droplayer)\n",
    "\n",
    "# sequence model\n",
    "input_2 = Input(shape=(max_len,))\n",
    "# embedding = Embedding(vocab_len, 50, )(input_2)\n",
    "embedding = Embedding(vocab_len, 100, embeddings_initializer=Constant(embedding_matrix), trainable=False)(input_2)\n",
    "droplayer_ = Dropout(0.5)(embedding)\n",
    "lstm = LSTM(256)(droplayer_)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([denselayer, lstm])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_len, activation='softmax')(decoder2)\n",
    "\n",
    "# tie it together [image, seq] [word]\n",
    "model = Model(inputs=[input_1, input_2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(trainX, trainY, epochs=25, batch_size=300, shuffle=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(15, 8))\n",
    "# loss = history.history['loss']\n",
    "# # val_loss = history.history['val_loss']\n",
    "# plt.plot(loss, label=\"train_loss\")\n",
    "# plt.plot(val_loss, label=\"val_loss\")\n",
    "# plt.legend()\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe396cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizr):\n",
    "    for word, index in tokenizr.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    \n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "        \n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        \n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        \n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        \n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        \n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601827fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "img_id, original_caption = random.choice(train_caption)\n",
    "predicted_caption = generate_desc(model, tokenizer, train_features[img_id], max_len)\n",
    "print(\"Original caption: \", original_caption)\n",
    "print(\"Predicted caption: \", predicted_caption)\n",
    "load_img(image_dataset_path + \"/\" + img_id + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df974d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model8k.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea14af91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_bleu\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# load the caption file & read it\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbleu_caption\u001b[39m(path):\n\u001b[0;32m      5\u001b[0m     \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# dictionary to store captions\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# load the caption file & read it\n",
    "def bleu_caption(path):\n",
    "    \n",
    "    # dictionary to store captions\n",
    "    captions_dict = {}\n",
    "    \n",
    "    # iterate through the file\n",
    "    for caption in open(path,encoding = 'utf-8'):\n",
    "    \n",
    "        # caption has format-> 1000268201_693b08cb0e.jpg#0  A child in a pink dress is climbing up a set of stairs in an entry way .\n",
    "        tokens = caption.split()\n",
    "        caption_id, caption_text = tokens[0].split('.')[0], tokens[1:]\n",
    "        caption_text = ' '.join(caption_text)\n",
    "        \n",
    "        # save it in the captions dictionary\n",
    "        captions_dict[caption_id] = caption_text\n",
    "        \n",
    "    return captions_dict\n",
    "\n",
    "bleu_dict = bleu_caption(caption_dataset_path)\n",
    "\n",
    "\n",
    "img_id, original_caption = random.choice(train_caption)\n",
    "load_img(image_dataset_path + \"/\" + img_id + \".jpg\")\n",
    "\n",
    "reference = bleu_dict[img_id].split(' ')\n",
    "candidate = generate_desc(model, tokenizer, train_features[img_id], max_len).split(' ')[1:-1]\n",
    "print(reference, candidate)\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
